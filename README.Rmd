---
title: FaaS - API modeling
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


![R](https://img.shields.io/badge/R%3E%3D-3.0.0-blue.svg)
![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)


**Repository for running scale modeling on FaaS**

Repository under license [Mozilla Public Version 2.0](https://www.mozilla.org/en-US/MPL/2.0/).

The script 'run_examples.R' has all the necessary code to run both examples covered in the tutorial below.

## Autentication

Available soon.

## I) How it works

You only need two functions to request a job, both inside the "aux" folder. **faas_api** is the main function. **load_libs** only load (and install if necessary) a few external libraries.

Let's load them:

```{r, eval=FALSE}
source("./aux/load_libs.R")
source("./aux/faas_api.R")
```

There are **4 basic arguments** to feed 'faas_api' function. We are going through all of them in this example and then will call the API. 



#### 1) Data List ['data_list']

A list of datasets to perform modeling;

Since we are dealing with time-series, the dataset *must contain a date column* (its name is not relevant, since we will automatically detect it). 

Moreover,  you must name every list element after the Y variable name.

Obs: Variables names (column names) that begin with numeric characters will be renamed to avoid computational issues. For example, variables "32", "156_y", "3_pim" will be displayed as "x32", "x156_y" and "x3_pim" at the end of the process. To avoid this correction, avoid beginning columns names with numeric characters;

Let us see two examples of data lists, one with 1 Y's and the other with multiple Y's
<br>

##### Example 1 data_list [single Y]:
```{r}

# Load a data frame with our data
dataset_1 <- readxl::read_excel("./inputs/dataset_1.xlsx")

# Put it inside a list (therefore, a 'data list')
# and name the list element with the name of the target variable
data_list <-  list(dataset_1)
names(data_list) <- c("fs_pim")

#Also, specify the date variable and its format 
date_variable <- "DATE_VARIABLE"
date_format <- '%Y-%m-%d'
```

<br>

##### Example 2 data_list [multiple Ys]:
```{r}

# Load a data frame with our data
dataset_1 <- readxl::read_excel("./inputs/dataset_1.xlsx")
dataset_2 <- readxl::read_excel("./inputs/dataset_2.xlsx")
dataset_3 <- readxl::read_excel("./inputs/dataset_3.xlsx")

# Put it inside a list (therefore, a 'data list')
# and name every list element with the name of the target variable
data_list <-  list(dataset_1, dataset_2, dataset_3)
names(data_list) <- c("fs_pim", "fs_pmc", "fs_pib")

# Also, specify the date variable and its format 
# (must have the same name in all datasets)
date_variable <- "DATE_VARIABLE"
date_format <- '%Y-%m-%d'
```
<br>


#### 2) **Model Specifications ['model_spec']**


Regardless of whether you are modeling one or multiple Ys, the model spec follows the same logic.  A list of desired modeling specification by the user:

* **n_steps**: forecast horizon that will be used in the cross-validation (if 3, 3 months ahead; if 12, 12 months ahead, etc.);

* **n_windows**: how many windows the size of 'Forecast Horizon' will be evaluated during cross-validation (CV);

* **seas.d**: if TRUE, it includes seasonal dummies in every estimation.

* **log**: if TRUE apply log transformation to the data;

* **accuracy_crit**: which criterion to measure the accuracy of the forecast during the CV (can be MPE, MAPE, or RMSE); 

* **exclusions**: restrictions on features in the same model;


<br>


The critical input we expect from users is the CV settings (n_steps and n_windows). In this example,  we set our modeling algorithm to perform a CV, which will evaluate forecasts 1 step ahead ('n_steps'), 12 times ('n_windows'). 

In this example, we keep other specifications at their default values. 
The accuracy criteria used to select the best models will be "MAPE". We'll be using data with log transformation, and proper seasonal dummies will be used in every estimation. Moreover, we avoid multicollinearity issues in linear models and apply three distinct methods of feature selection. In the last section of this file, we present more advanced settings examples. 

```{r, results=FALSE}

## EXAMPLE 1
model_spec <- list(log = TRUE,
                   seas.d = TRUE,
                   n_steps = 1,
                   n_windows = 12,
                   n_best = 20,
                   accuracy_crit = "MAPE",
                   info_crit = "AIC",
                   exclusions = list(),
                   selection_methods = list(
                     lasso = TRUE,
                     rf = TRUE,
                     corr = TRUE,
                     apply.collinear = c("corr","rf","lasso","no_reduction")))

```
<br>



#### 3) Project ID ['project_id']

Define a project name. It accepts character and numeric inputs. Special characters will be removed.

```{r}
project_id <- "example_project"
```

#### 4) User Email ['user_mail']

Set the user email. We are going to use it to let you know when the modeling is over.
```{r}
user_email <- "user@domain.com"
```

#### 5) Send job request

Everything looks nice? Great! Now you can send **FaaS API** request:

```{r, eval=FALSE}
faas_api(data_list, date_variable, date_format, model_spec, project_id, user_email) 
```


## II) Advanced Options
In this section, we change some the default values of the  **model_spec**. *Only advanced users should edit them: make sure you understand the implications before changing them.*

The accuracy criteria used to select the best models will be "RMSE". We're not applying log transformation on data. Moreover,  we also make use of the  **'exclusions'** option :

```{r, results=FALSE}

## EXAMPLE 2
model_spec <- list(log = FALSE,
                   seas.d = TRUE,
                   n_steps = 1,
                   n_windows = 12,
                   n_best = 20,
                   accuracy_crit = "RMSE",
                   info_crit = "AIC",
                   exclusions = list(c("fs_massa_real", "fs_rend_medio"),
                                     c("fs_pop_ea", "fs_pop_des", "fs_pop_ocu")),
                   selection_methods = list(
                     lasso = TRUE,
                     rf = TRUE,
                     corr = TRUE,
                     apply.collinear = c("corr","rf","lasso","no_reduction")))

```
<br>

By setting **exclusions** this way, we add the restriction where the features/variables in a group can not appear together in the same model. Pay attention to the following lines:

```{r, results=FALSE}

exclusions = list(c("fs_massa_real", "fs_rend_medio"),
                  c("fs_pop_ea", "fs_pop_des", "fs_pop_ocu"))

```

This list implies that we will never see "fs_massa_real" and "fs_rend_medio" in the same model. The same is true for the second restriction group: we will never estimate models that simultaneously include "fs_pop_ea", with either "fs_pop_des" and "fs_pop_ocu", and so on.

```{r, include=FALSE}

# With the **'golden_variables'** argument, we can guarantee that at least some of best models contain one or both of the 'golden' ones; 
golden_variables = c("fs_pmc", "fs_ici")

```
<br>

The **selection methods**  determine feature selection algorithms that will be used when it comes to big datasets (one with a large number of explanatory features).  More precisely, if the number of features in the dataset exceeds 14, feature selection methods will reduce dimensionality, guaranteeing the best results in a much more efficient way. In this example, we turn off the Lasso method and work only with Random Forestation and the correlation approach.

```{r}
 selection_methods = list(
                     lasso = FALSE,
                     rf = TRUE,
                     corr = TRUE,
                     apply.collinear = "")
```
<br>


